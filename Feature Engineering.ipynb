{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240cff77",
   "metadata": {},
   "source": [
    "Assignment Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2789e76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. What is a parameter?\\n\\n-> A parameter refers to a configuration variable that is learned from data during the training process.\\n\\nTypes of parameters:\\n\\na. Model Parameters:\\n\\n-> Learned by the algorithm from training data.\\n-> Define how the model makes predictions.\\n\\nExamples:\\nWeights and biases in a neural network\\nCoefficients in linear regression\\nSupport vectors in SVM\\n\\nb. Hyperparameters(related, but different):\\n\\n-> Set before training and not learned from the data.\\n\\nExamples: learning rate, number of layers, regularization strength\\n\\nExample:\\nIn linear regression:\\nThe model predicts: output=weight*input+bias\\nHere, weight and bias are parameters that are learned by minimizing the error on training data.\\n\\n2. What is correlation?\\n What does negative correlation mean?\\n\\n-> Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\\n\\nIt ranges from -1 to +1.\\nA correlation near +1 means a strong positive relationship.\\nA correlation near -1 means a strong negative relationship.\\nA correlation near 0 means no linear relationship.\\n\\nA negative correlation means that as one variable increases, the other decreases.\\nExample: The more time spent watching TV, the lower the test scores might be.\\nIf the correlation is -0.8, the relationship is strong and inverse.\\n\\n3. Define Machine Learning. What are the main components in Machine Learning?\\n\\n-> Machine Learning(ML) is a branch of artificial intelligence that focuses on \\nbuilding systems that can learn from data, identify patterns, and make decisions with minimal human intervention.\\n\\nMain Components in Machine Learning:\\n\\na. Data:\\n\\n-> Raw input used to train and evaluate the model.\\n-> Includes features(inputs) and labels(outputs) in supervised learning.\\n\\nb. Model:\\n\\n-> A mathematical representation or algorithm that maps inputs to outputs.\\n\\nExamples: Linear regression, decision tree, neural network.\\n\\nc. Algorithm:\\n\\n-> The procedure used to train the model by adjusting parameters.\\n\\nExample: Gradient Descent, K-Means Clustering, Backpropagation.\\n\\nd. Loss Function(or Cost Function):\\n\\n-> Measures how well the model\\'s predictions match the actual results.\\n\\nCommon functions: Mean Squared Error, Cross-Entropy Loss.\\n\\ne. Training:\\n\\n-> The process of feeding data to the algorithm so it learns patterns.\\n-> Adjusts model parameters to minimize the loss.\\n\\nf. Evaluation:\\n\\n-> Assessing model performance using metrics like accuracy, precision, recall, or RMSE.\\n-> Typically done on unseen(test or validation) data.\\n\\ng. Prediction(Inference):\\n\\n-> Using the trained model to make predictions on new data.\\n\\n4. How does loss value help in determining whether the model is good or not?\\n\\n-> The loss value is a key indicator of how well a machine learning model is performing.\\n\\nIt helps determine model quality:\\n\\na. Lower Loss=Better Predictions:\\n-> A small loss value means the model\\'s predictions are close to the true values.\\n-> A high loss indicates poor performance.\\n\\nb. Tracks Learning Progress:\\n-> During training, we monitor the loss after each iteration or epoch.\\n-> A decreasing loss shows the model is learning.\\n\\nc. Compare Models:\\nLoss allows for objective comparison between different models or configurations.\\n\\nd. Avoid Overfitting:\\nIf training loss is low but validation loss is high, the model may be overfitting \\n(memorizing the training data rather than learning general patterns).\\n\\n5. What are continuous and categorical variables?\\n\\n-> Continuous Variables:\\na. These are quantitative variables that can take any numerical value within a range.\\nb. They are measurable and can include decimals or fractions.\\n\\nExamples:\\nHeight(e.g., 172.5 cm)\\nTemperature(e.g., 36.6 °C)\\nSalary(e.g., ₹55,000.75)\\n\\nCategorical Variables:\\na. These are qualitative variables that represent categories or groups.\\nb. They have discrete values and cannot be measured on a numerical scale.\\n\\nTypes:\\nNominal: No inherent order(e.g., gender, color)\\nOrdinal: With an order or ranking(e.g., education level, satisfaction rating)\\n\\nExamples:\\nGender(Male, Female)\\nMarital Status(Single, Married)\\nShirt Size(Small, Medium, Large)\\n\\n6. How do we handle categorical variables in Machine Learning? What are the common techniques?\\n\\n-> Handling categorical variables is essential in machine learning because most algorithms require numerical input. \\n\\nCommon techniques include:\\na. Label Encoding:\\n-> Converts each category into a unique integer.\\n-> Best for ordinal variables(where order matters).\\n\\nExample:\\nfrom sklearn.preprocessing import LabelEncoder\\nle=LabelEncoder()\\ndata[\\'Size\\']=le.fit_transform(data[\\'Size\\'])  # Small=2, Medium=1, Large=0\\n\\nb. One-Hot Encoding:\\n-> Creates a binary column for each category.\\n-> Ideal for nominal variables (no order).\\n\\nExample:\\nimport pandas as pd\\npd.get_dummies(data[\\'Color\\'],prefix=\\'Color\\')   # Red -> [1, 0, 0], Green -> [0, 1, 0], Blue -> [0, 0, 1]\\n\\nc. Ordinal Encoding: \\n-> Assigns ordered integers to categories manually or automatically.\\n-> Used when categories have ranked relationships.\\n\\nExample:\\ndata[\\'Education\\']=data[\\'Education\\'].map({\\n    \\'High School\\':1,\\n    \\'Bachelor\\':2,\\n    \\'Master\\':3,\\n    \\'PhD\\':4\\n})\\n\\nd. Frequency or Count Encoding:\\n-> Replaces categories with frequency counts.\\n-> Useful for high-cardinality features.\\n\\nExample:\\ndata[\\'Category\\']=data[\\'Category\\'].map(data[\\'Category\\'].value_counts())\\n\\ne. Target Encoding:\\n-> Replaces categories with the mean of the target variable for that category.\\n-> Often used in regression tasks.\\n\\nExample:\\nmean_target=data.groupby(\\'Category\\')[\\'Target\\'].mean()\\ndata[\\'Category_encoded\\']=data[\\'Category\\'].map(mean_target)\\n\\n7. What do you mean by training and testing a dataset?\\n\\n-> Training a dataset means using a portion of our data to teach the machine learning model how to \\nmake predictions or identify patterns. The model learns by adjusting itself to minimize errors on this training data.\\n\\nTesting a dataset means using a separate portion of data(not seen by the model during training) to evaluate how \\nwell the trained model performs on new, unseen data. This helps check if the model can generalize \\nbeyond just memorizing the training examples.\\n\\n8. What is sklearn.preprocessing?\\n\\n-> sklearn.preprocessing is a module in the Scikit-learn(sklearn) library that provides tools \\nfor preparing(or preprocessing) data before feeding it into a machine learning model.\\n\\nThis module helps to:\\n\\na. Scale features(e.g., standardize values)\\nb. Encode categorical variables\\nc. Normalize data\\nd. Handle missing or unevenly distributed data\\n\\n9. What is a Test set?\\n\\n-> A test set is a portion of our dataset that is used to evaluate the final performance \\nof a trained machine learning model.\\n\\nPurpose of test set:\\na. To measure how well the model generalizes to new, unseen data.\\nb. It gives an unbiased evaluation of the model\\'s accuracy, precision, recall, etc.\\n\\n10. How do we split data for model fitting (training and testing) in Python?\\n How do you approach a Machine Learning problem?\\n\\n-> We use the train_test_split function from Scikit-learn(sklearn) to divide the dataset:\\nfrom sklearn.model_selection import train_test_split\\n# Assuming X=features, y=labels/target\\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\\n\\nParameters:\\ntest_size=0.2 -> 20% data for testing, 80% for training.\\nrandom_state=42 -> ensures reproducibility.\\nshuffle=True by default -> shuffles before splitting.\\n\\nWe approach a machine learning problem as follows:\\n\\na. Understand the Problem:\\n-> Define the goal clearly.\\n-> Know the type: classification, regression, clustering, etc.\\n\\nb. Collect Data:\\nGather relevant data from databases, APIs, files, etc.\\n\\nc. Explore and Preprocess Data:\\n-> Handle missing values.\\n-> Remove duplicates.\\n-> Treat outliers.\\n-> Convert categorical variables(e.g., one-hot encoding):\\n-> Feature scaling(standardization/normalization).\\n\\nd. Split Data:\\n-> Split into training and test sets(e.g., 80/20).\\n-> Optionally, use a validation set or cross-validation.\\n\\ne. Choose and Train Model:\\n-> Pick a suitable ML algorithm(e.g., Linear Regression, Random Forest).\\n-> Fit the model to the training data.\\n\\nf. Evaluate Model:\\n-> Use metrics like accuracy, F1-score, RMSE, etc.\\n-> Evaluate on test data to check generalization.\\n\\ng. Tune and Improve:\\n-> Hyperparameter tuning(GridSearchCV, RandomizedSearchCV).\\n-> Feature engineering or model stacking.\\n-> Retrain and re-evaluate.\\n\\n11. Why do we have to perform EDA before fitting a model to the data?\\n\\n-> Exploratory Data Analysis(EDA) is a crucial step before fitting a model because it helps us understand \\nthe structure, patterns, and quality of the data. \\n\\nEDA is important because:\\n\\na. Understand Data Distribution:\\n-> Helps us see how features are distributed.\\n-> Identify skewed data, imbalances in target variables, etc.\\n\\nb. Detect Missing Values:\\n-> We can identify which columns have missing or null values.\\n-> Allows us to decide on imputation or removal.\\n\\nc. Identify Outliers:\\n-> Outliers can distort model training and performance.\\n-> Visualization(box plots, scatter plots) helps detect them.\\n\\nd. Discover Relationships:\\n-> Correlation analysis can show which features influence the target.\\n-> Helps in feature selection and engineering.\\n\\ne. Choose the Right Model:\\nKnowing whether the target is categorical or continuous influences model choice(classification vs. regression).\\n\\nf. Improve Model Performance:\\n-> Clean, well-understood data leads to more accurate and interpretable models.\\n-> Prevents garbage in, garbage out.\\n\\ng. Guide Preprocessing Steps:\\nSuggests what encoding, scaling, or transformation is needed.\\n\\n12. What is correlation?\\n\\n-> Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\\n\\nIt ranges from -1 to +1.\\nA correlation near +1 means a strong positive relationship.\\nA correlation near -1 means a strong negative relationship.\\nA correlation near 0 means no linear relationship.\\n\\n13. What does negative correlation mean?\\n\\n-> A negative correlation means that as one variable increases, the other decreases.\\nExample: The more time spent watching TV, the lower the test scores might be.\\nIf the correlation is -0.8, the relationship is strong and inverse.\\n\\n14. How can you find correlation between variables in Python?\\n\\n-> We can find the correlation between variables in Python using the pandas library \\nand visualize it using seaborn or matplotlib.\\n\\na. Using pandas.corr():\\n\\nimport pandas as pd\\n# Sample DataFrame\\ndata = {\\n    \\'height\\':[150,160,170,180,190],\\n    \\'weight\\':[50,60,70,80,90],\\n    \\'age\\':[22,25,30,35,40]\\n}\\ndf=pd.DataFrame(data)\\n# Calculate correlation matrix\\ncorrelation_matrix=df.corr()\\nprint(correlation_matrix)\\n\\nThis gives Pearson correlation by default(range: -1 to 1).\\n\\nb. Visualize with Heatmap(seaborn):\\n\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nsns.heatmap(correlation_matrix,annot=True,cmap=\\'coolwarm\\')\\nplt.title(\"Correlation Matrix\")\\nplt.show()\\n\\nc. Other Correlation Methods: \\n\\nWe can also specify methods like:\\n\\n\"pearson\" - default, for linear relationships\\n\"kendall\" - for ordinal or non-parametric data\\n\"spearman\" - for ranked data\\n\\ndf.corr(method=\\'spearman\\')\\n\\n15. What is causation? Explain difference between correlation and causation with an example.\\n\\n-> Causation means that one variable directly affects another - a change in one variable produces a change in the other.\\ncausation=cause and effect.\\n\\nDifference between correlation and causation are:\\n\\nCorrelation:\\na. Shows a statistical relationship between two variables.\\nb. Change in one variable is associated with change in another.\\nc. Does not imply one variable causes the other to change.\\nd. Can be positive, negative, or zero.\\ne. May be due to a third(confounding) variable.\\n\\nExample: Ice cream sales and drowning rates both increase in summer(due to heat).\\n\\nCausation:\\na. Indicates a cause-and-effect relationship.\\nb. Change in one variable directly causes change in another.\\nc. Requires controlled experiments or strong evidence.\\nd. Always has a directional influence(from cause to effect).\\n\\nExample: Smoking causes lung cancer(proven by medical research).\\n\\n16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\\n\\n-> An optimizer is an algorithm or method used in machine learning and deep learning to adjust the model\\'s \\nparameters(like weights and biases) during training, with the goal of minimizing the loss function. \\nThe loss function measures how well the model is performing - the optimizer tries to find the best parameters \\nthat reduce this error.\\n\\nDifferent types of optimizers:\\na. Gradient Descent(GD):\\n-> The most basic optimizer.\\n-> Updates model parameters by moving them in the direction of the negative gradient of the loss function.\\n-> Calculates gradients on the entire dataset for each update.\\n\\nExample: Used in simple linear regression.\\n\\nb. Stochastic Gradient Descent(SGD):\\n-> Similar to Gradient Descent but updates parameters using one training example at a time.\\n-> Faster updates but noisier(less stable).\\n-> Useful for large datasets.\\n\\nExample: Used in neural networks where data is too big to process at once.\\n\\nc. Mini-batch Gradient Descent:\\n-> A hybrid between GD and SGD.\\n-> Updates parameters based on a small random batch of data(mini-batch) instead of the whole dataset or a single example.\\n-> Balances speed and stability.\\n\\nExample: Commonly used in deep learning frameworks like TensorFlow and PyTorch.\\n\\nd. Momentum:\\n-> Accelerates SGD by adding a momentum term that helps the optimizer keep moving in the same direction.\\n-> Helps avoid getting stuck in local minima and speeds up convergence.\\n\\nExample: Often combined with SGD in training deep neural networks.\\n\\ne. RMSprop(Root Mean Square Propagation):\\n-> Adjusts the learning rate adaptively for each parameter based on recent gradients.\\n-> Divides the learning rate by a moving average of the magnitudes of recent gradients.\\n-> Works well for non-stationary objectives(changing loss landscape).\\n\\nExample: Popular for recurrent neural networks(RNNs).\\n\\nf. Adam(Adaptive Moment Estimation):\\n-> Combines ideas from Momentum and RMSprop.\\n-> Maintains moving averages of both gradients and squared gradients.\\n-> Automatically adjusts learning rates for each parameter.\\n-> Often works well in practice and is widely used.\\n\\nExample: Default optimizer for many deep learning tasks.\\n\\n17. What is sklearn.linear_model?\\n\\n-> sklearn.linear_model is a module in the scikit-learn Python library that provides classes \\nand functions to implement various linear models for regression and classification tasks.\\n\\nIt offers:\\na. It contains algorithms that model the relationship between input features and \\na target variable assuming a linear relationship.\\nb. It supports both regression(predicting continuous values) and classification(predicting categories) problems.\\nc. The module includes simple and advanced linear models, along with tools for regularization to prevent overfitting.\\n\\nExample:\\nfrom sklearn.linear_model import LinearRegression\\nmodel=LinearRegression()\\nmodel.fit(X_train,y_train)  # Train the model\\npredictions=model.predict(X_test)  # Predict outcomes\\n\\n18. What does model.fit() do? What arguments must be given?\\n\\n-> The fit() method in machine learning models (including scikit-learn models) is used to train the model \\non the provided data. When we call model.fit(), the algorithm:\\na. Learns the relationship between the input features and the target variable.\\nb. Finds the best parameters(e.g., weights in linear regression) that minimize the error or loss function.\\nc. Prepares the model to make predictions on new/unseen data.\\n\\nThe most common arguments passed to fit() are:\\n\\na. X — The input data(features):\\n\\n-> Usually a 2D array-like structure(e.g., NumPy array, pandas DataFrame).\\n-> Shape: (number of samples, number of features).\\n\\nb. y — The target variable (labels or values to predict):\\n\\n-> 1D array-like for regression or binary/multiclass classification.\\n-> Shape: (number of samples,).\\n\\nExample:\\nfrom sklearn.linear_model import LinearRegression\\nimport numpy as np\\n# Sample data\\nX=np.array([[1,2],[2,3],[3,4],[4,5]])  # Features\\ny=np.array([3,5,7,9])                  # Target variable\\nmodel=LinearRegression()\\nmodel.fit(X,y)  # Train the model on X and y\\n\\n19. What does model.predict() do? What arguments must be given?\\n\\n-> The predict() method is used to make predictions using the trained machine learning model. \\nAfter we have trained our model with fit(), calling predict() applies the learned patterns \\nto new input data to estimate the output(target values or classes).\\n\\nThe most common arguments passed to predict() are:\\na. X - The input data(features) for which we want predictions:\\n-> Typically a 2D array-like structure(NumPy array, pandas DataFrame).\\n-> Shape: (number of samples, number of features).\\nb. No target(y) is needed because we want the model to predict these.\\n\\nExample:\\nfrom sklearn.linear_model import LinearRegression\\nimport numpy as np\\n# Training data\\nX_train=np.array([[1,2],[2,3],[3,4]])\\ny_train=np.array([3,5,7])\\n# Train the model\\nmodel=LinearRegression()\\nmodel.fit(X_train,y_train)\\n# New data for prediction\\nX_new=np.array([[4,5],[5,6]])\\n# Predict target values for new data\\npredictions=model.predict(X_new)\\nprint(predictions)\\n\\n20. What are continuous and categorical variables?\\n\\n-> Continuous Variables:\\na. These are quantitative variables that can take any numerical value within a range.\\nb. They are measurable and can include decimals or fractions.\\n\\nExamples:\\nHeight(e.g., 172.5 cm)\\nTemperature(e.g., 36.6 °C)\\nSalary(e.g., ₹55,000.75)\\n\\nCategorical Variables:\\na. These are qualitative variables that represent categories or groups.\\nb. They have discrete values and cannot be measured on a numerical scale.\\n\\nTypes:\\nNominal: No inherent order(e.g., gender, color)\\nOrdinal: With an order or ranking(e.g., education level, satisfaction rating)\\n\\nExamples:\\nGender(Male, Female)\\nMarital Status(Single, Married)\\nShirt Size(Small, Medium, Large)\\n\\n21. What is feature scaling? How does it help in Machine Learning?\\n\\n-> Feature scaling is the process of normalizing or standardizing the range of independent variables(features) \\nin our data. It transforms features so they have similar scales or distributions.\\n\\nIt helps in machine learning:\\na. Many ML algorithms(like gradient descent, k-nearest neighbors, SVM, and neural networks) \\nare sensitive to the scale of features.\\nb. Features with larger ranges can dominate the learning process, causing biased results.\\nc. Scaling ensures all features contribute equally, improving:\\n-> Model convergence speed(especially for gradient-based methods).\\n-> Model performance and accuracy.\\n-> Interpretability when comparing feature effects.\\n\\n22. How do we perform scaling in Python?\\n\\n-> We can perform feature scaling easily in Python using scikit-learn\\'s preprocessing module. \\n\\nThe common methods are:\\n\\na. Min-Max Scaling:\\nScales data to a range between 0 and 1.\\n\\nfrom sklearn.preprocessing import MinMaxScaler\\nimport numpy as np\\ndata=np.array([[10, 200],\\n                 [20, 300],\\n                 [30, 400]])\\nscaler=MinMaxScaler()\\nscaled_data=scaler.fit_transform(data)\\nprint(scaled_data)\\n\\nb. Standardization(Z-score scaling):\\nCenters data to mean zero and scales to unit variance.\\n\\nfrom sklearn.preprocessing import StandardScaler\\nimport numpy as np\\ndata=np.array([[10, 200],\\n                 [20, 300],\\n                 [30, 400]])\\nscaler=StandardScaler()\\nscaled_data=scaler.fit_transform(data)\\nprint(scaled_data)\\n\\nc. Normalizer:\\n\\nScales each sample(row) to have unit norm(length 1), converting it into a unit vector.\\n\\nfrom sklearn.preprocessing import Normalizer\\nimport numpy as np\\ndata=np.array([[4, 1],\\n                 [1, 2],\\n                 [3, 3]])\\nscaler=Normalizer()\\nnormalized_data=scaler.fit_transform(data)\\nprint(normalized_data)\\n\\nSteps to perform scaling:\\na. Create the scaler object.\\nb. Apply .fit_transform() on our data.\\nc. Get scaled data ready for modeling.\\n\\n23. What is sklearn.preprocessing?\\n\\n-> sklearn.preprocessing is a module in the Scikit-learn(sklearn) library that provides tools \\nfor preparing(or preprocessing) data before feeding it into a machine learning model.\\n\\nThis module helps to:\\n\\na. Scale features(e.g., standardize values)\\nb. Encode categorical variables\\nc. Normalize data\\nd. Handle missing or unevenly distributed data\\n\\n24. How do we split data for model fitting (training and testing) in Python?\\n\\n-> We use the train_test_split function from Scikit-learn(sklearn) to divide the dataset:\\nfrom sklearn.model_selection import train_test_split\\n# Assuming X=features, y=labels/target\\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\\n\\nParameters:\\ntest_size=0.2 -> 20% data for testing, 80% for training.\\nrandom_state=42 -> ensures reproducibility.\\nshuffle=True by default -> shuffles before splitting.\\n\\n25. Explain data encoding?\\n\\n-> Data encoding in machine learning is the process of converting categorical variables(non-numeric data) \\ninto a numeric format so that algorithms can process them effectively. Since most machine learning models \\nwork with numerical data, encoding is a crucial step in data preprocessing.\\n\\nEncoding is important because:\\na. Algorithms like linear regression, logistic regression, SVMs, etc., require numeric input.\\nb. Encoding allows models to interpret categories meaningfully.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. What is a parameter?\n",
    "\n",
    "-> A parameter refers to a configuration variable that is learned from data during the training process.\n",
    "\n",
    "Types of parameters:\n",
    "\n",
    "a. Model Parameters:\n",
    "\n",
    "-> Learned by the algorithm from training data.\n",
    "-> Define how the model makes predictions.\n",
    "\n",
    "Examples:\n",
    "Weights and biases in a neural network\n",
    "Coefficients in linear regression\n",
    "Support vectors in SVM\n",
    "\n",
    "b. Hyperparameters(related, but different):\n",
    "\n",
    "-> Set before training and not learned from the data.\n",
    "\n",
    "Examples: learning rate, number of layers, regularization strength\n",
    "\n",
    "Example:\n",
    "In linear regression:\n",
    "The model predicts: output=weight*input+bias\n",
    "Here, weight and bias are parameters that are learned by minimizing the error on training data.\n",
    "\n",
    "2. What is correlation?\n",
    " What does negative correlation mean?\n",
    "\n",
    "-> Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
    "\n",
    "It ranges from -1 to +1.\n",
    "A correlation near +1 means a strong positive relationship.\n",
    "A correlation near -1 means a strong negative relationship.\n",
    "A correlation near 0 means no linear relationship.\n",
    "\n",
    "A negative correlation means that as one variable increases, the other decreases.\n",
    "Example: The more time spent watching TV, the lower the test scores might be.\n",
    "If the correlation is -0.8, the relationship is strong and inverse.\n",
    "\n",
    "3. Define Machine Learning. What are the main components in Machine Learning?\n",
    "\n",
    "-> Machine Learning(ML) is a branch of artificial intelligence that focuses on \n",
    "building systems that can learn from data, identify patterns, and make decisions with minimal human intervention.\n",
    "\n",
    "Main Components in Machine Learning:\n",
    "\n",
    "a. Data:\n",
    "\n",
    "-> Raw input used to train and evaluate the model.\n",
    "-> Includes features(inputs) and labels(outputs) in supervised learning.\n",
    "\n",
    "b. Model:\n",
    "\n",
    "-> A mathematical representation or algorithm that maps inputs to outputs.\n",
    "\n",
    "Examples: Linear regression, decision tree, neural network.\n",
    "\n",
    "c. Algorithm:\n",
    "\n",
    "-> The procedure used to train the model by adjusting parameters.\n",
    "\n",
    "Example: Gradient Descent, K-Means Clustering, Backpropagation.\n",
    "\n",
    "d. Loss Function(or Cost Function):\n",
    "\n",
    "-> Measures how well the model's predictions match the actual results.\n",
    "\n",
    "Common functions: Mean Squared Error, Cross-Entropy Loss.\n",
    "\n",
    "e. Training:\n",
    "\n",
    "-> The process of feeding data to the algorithm so it learns patterns.\n",
    "-> Adjusts model parameters to minimize the loss.\n",
    "\n",
    "f. Evaluation:\n",
    "\n",
    "-> Assessing model performance using metrics like accuracy, precision, recall, or RMSE.\n",
    "-> Typically done on unseen(test or validation) data.\n",
    "\n",
    "g. Prediction(Inference):\n",
    "\n",
    "-> Using the trained model to make predictions on new data.\n",
    "\n",
    "4. How does loss value help in determining whether the model is good or not?\n",
    "\n",
    "-> The loss value is a key indicator of how well a machine learning model is performing.\n",
    "\n",
    "It helps determine model quality:\n",
    "\n",
    "a. Lower Loss=Better Predictions:\n",
    "-> A small loss value means the model's predictions are close to the true values.\n",
    "-> A high loss indicates poor performance.\n",
    "\n",
    "b. Tracks Learning Progress:\n",
    "-> During training, we monitor the loss after each iteration or epoch.\n",
    "-> A decreasing loss shows the model is learning.\n",
    "\n",
    "c. Compare Models:\n",
    "Loss allows for objective comparison between different models or configurations.\n",
    "\n",
    "d. Avoid Overfitting:\n",
    "If training loss is low but validation loss is high, the model may be overfitting \n",
    "(memorizing the training data rather than learning general patterns).\n",
    "\n",
    "5. What are continuous and categorical variables?\n",
    "\n",
    "-> Continuous Variables:\n",
    "a. These are quantitative variables that can take any numerical value within a range.\n",
    "b. They are measurable and can include decimals or fractions.\n",
    "\n",
    "Examples:\n",
    "Height(e.g., 172.5 cm)\n",
    "Temperature(e.g., 36.6 °C)\n",
    "Salary(e.g., ₹55,000.75)\n",
    "\n",
    "Categorical Variables:\n",
    "a. These are qualitative variables that represent categories or groups.\n",
    "b. They have discrete values and cannot be measured on a numerical scale.\n",
    "\n",
    "Types:\n",
    "Nominal: No inherent order(e.g., gender, color)\n",
    "Ordinal: With an order or ranking(e.g., education level, satisfaction rating)\n",
    "\n",
    "Examples:\n",
    "Gender(Male, Female)\n",
    "Marital Status(Single, Married)\n",
    "Shirt Size(Small, Medium, Large)\n",
    "\n",
    "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "\n",
    "-> Handling categorical variables is essential in machine learning because most algorithms require numerical input. \n",
    "\n",
    "Common techniques include:\n",
    "a. Label Encoding:\n",
    "-> Converts each category into a unique integer.\n",
    "-> Best for ordinal variables(where order matters).\n",
    "\n",
    "Example:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "data['Size']=le.fit_transform(data['Size'])  # Small=2, Medium=1, Large=0\n",
    "\n",
    "b. One-Hot Encoding:\n",
    "-> Creates a binary column for each category.\n",
    "-> Ideal for nominal variables (no order).\n",
    "\n",
    "Example:\n",
    "import pandas as pd\n",
    "pd.get_dummies(data['Color'],prefix='Color')   # Red -> [1, 0, 0], Green -> [0, 1, 0], Blue -> [0, 0, 1]\n",
    "\n",
    "c. Ordinal Encoding: \n",
    "-> Assigns ordered integers to categories manually or automatically.\n",
    "-> Used when categories have ranked relationships.\n",
    "\n",
    "Example:\n",
    "data['Education']=data['Education'].map({\n",
    "    'High School':1,\n",
    "    'Bachelor':2,\n",
    "    'Master':3,\n",
    "    'PhD':4\n",
    "})\n",
    "\n",
    "d. Frequency or Count Encoding:\n",
    "-> Replaces categories with frequency counts.\n",
    "-> Useful for high-cardinality features.\n",
    "\n",
    "Example:\n",
    "data['Category']=data['Category'].map(data['Category'].value_counts())\n",
    "\n",
    "e. Target Encoding:\n",
    "-> Replaces categories with the mean of the target variable for that category.\n",
    "-> Often used in regression tasks.\n",
    "\n",
    "Example:\n",
    "mean_target=data.groupby('Category')['Target'].mean()\n",
    "data['Category_encoded']=data['Category'].map(mean_target)\n",
    "\n",
    "7. What do you mean by training and testing a dataset?\n",
    "\n",
    "-> Training a dataset means using a portion of our data to teach the machine learning model how to \n",
    "make predictions or identify patterns. The model learns by adjusting itself to minimize errors on this training data.\n",
    "\n",
    "Testing a dataset means using a separate portion of data(not seen by the model during training) to evaluate how \n",
    "well the trained model performs on new, unseen data. This helps check if the model can generalize \n",
    "beyond just memorizing the training examples.\n",
    "\n",
    "8. What is sklearn.preprocessing?\n",
    "\n",
    "-> sklearn.preprocessing is a module in the Scikit-learn(sklearn) library that provides tools \n",
    "for preparing(or preprocessing) data before feeding it into a machine learning model.\n",
    "\n",
    "This module helps to:\n",
    "\n",
    "a. Scale features(e.g., standardize values)\n",
    "b. Encode categorical variables\n",
    "c. Normalize data\n",
    "d. Handle missing or unevenly distributed data\n",
    "\n",
    "9. What is a Test set?\n",
    "\n",
    "-> A test set is a portion of our dataset that is used to evaluate the final performance \n",
    "of a trained machine learning model.\n",
    "\n",
    "Purpose of test set:\n",
    "a. To measure how well the model generalizes to new, unseen data.\n",
    "b. It gives an unbiased evaluation of the model's accuracy, precision, recall, etc.\n",
    "\n",
    "10. How do we split data for model fitting (training and testing) in Python?\n",
    " How do you approach a Machine Learning problem?\n",
    "\n",
    "-> We use the train_test_split function from Scikit-learn(sklearn) to divide the dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Assuming X=features, y=labels/target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "Parameters:\n",
    "test_size=0.2 -> 20% data for testing, 80% for training.\n",
    "random_state=42 -> ensures reproducibility.\n",
    "shuffle=True by default -> shuffles before splitting.\n",
    "\n",
    "We approach a machine learning problem as follows:\n",
    "\n",
    "a. Understand the Problem:\n",
    "-> Define the goal clearly.\n",
    "-> Know the type: classification, regression, clustering, etc.\n",
    "\n",
    "b. Collect Data:\n",
    "Gather relevant data from databases, APIs, files, etc.\n",
    "\n",
    "c. Explore and Preprocess Data:\n",
    "-> Handle missing values.\n",
    "-> Remove duplicates.\n",
    "-> Treat outliers.\n",
    "-> Convert categorical variables(e.g., one-hot encoding):\n",
    "-> Feature scaling(standardization/normalization).\n",
    "\n",
    "d. Split Data:\n",
    "-> Split into training and test sets(e.g., 80/20).\n",
    "-> Optionally, use a validation set or cross-validation.\n",
    "\n",
    "e. Choose and Train Model:\n",
    "-> Pick a suitable ML algorithm(e.g., Linear Regression, Random Forest).\n",
    "-> Fit the model to the training data.\n",
    "\n",
    "f. Evaluate Model:\n",
    "-> Use metrics like accuracy, F1-score, RMSE, etc.\n",
    "-> Evaluate on test data to check generalization.\n",
    "\n",
    "g. Tune and Improve:\n",
    "-> Hyperparameter tuning(GridSearchCV, RandomizedSearchCV).\n",
    "-> Feature engineering or model stacking.\n",
    "-> Retrain and re-evaluate.\n",
    "\n",
    "11. Why do we have to perform EDA before fitting a model to the data?\n",
    "\n",
    "-> Exploratory Data Analysis(EDA) is a crucial step before fitting a model because it helps us understand \n",
    "the structure, patterns, and quality of the data. \n",
    "\n",
    "EDA is important because:\n",
    "\n",
    "a. Understand Data Distribution:\n",
    "-> Helps us see how features are distributed.\n",
    "-> Identify skewed data, imbalances in target variables, etc.\n",
    "\n",
    "b. Detect Missing Values:\n",
    "-> We can identify which columns have missing or null values.\n",
    "-> Allows us to decide on imputation or removal.\n",
    "\n",
    "c. Identify Outliers:\n",
    "-> Outliers can distort model training and performance.\n",
    "-> Visualization(box plots, scatter plots) helps detect them.\n",
    "\n",
    "d. Discover Relationships:\n",
    "-> Correlation analysis can show which features influence the target.\n",
    "-> Helps in feature selection and engineering.\n",
    "\n",
    "e. Choose the Right Model:\n",
    "Knowing whether the target is categorical or continuous influences model choice(classification vs. regression).\n",
    "\n",
    "f. Improve Model Performance:\n",
    "-> Clean, well-understood data leads to more accurate and interpretable models.\n",
    "-> Prevents garbage in, garbage out.\n",
    "\n",
    "g. Guide Preprocessing Steps:\n",
    "Suggests what encoding, scaling, or transformation is needed.\n",
    "\n",
    "12. What is correlation?\n",
    "\n",
    "-> Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
    "\n",
    "It ranges from -1 to +1.\n",
    "A correlation near +1 means a strong positive relationship.\n",
    "A correlation near -1 means a strong negative relationship.\n",
    "A correlation near 0 means no linear relationship.\n",
    "\n",
    "13. What does negative correlation mean?\n",
    "\n",
    "-> A negative correlation means that as one variable increases, the other decreases.\n",
    "Example: The more time spent watching TV, the lower the test scores might be.\n",
    "If the correlation is -0.8, the relationship is strong and inverse.\n",
    "\n",
    "14. How can you find correlation between variables in Python?\n",
    "\n",
    "-> We can find the correlation between variables in Python using the pandas library \n",
    "and visualize it using seaborn or matplotlib.\n",
    "\n",
    "a. Using pandas.corr():\n",
    "\n",
    "import pandas as pd\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'height':[150,160,170,180,190],\n",
    "    'weight':[50,60,70,80,90],\n",
    "    'age':[22,25,30,35,40]\n",
    "}\n",
    "df=pd.DataFrame(data)\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix=df.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "This gives Pearson correlation by default(range: -1 to 1).\n",
    "\n",
    "b. Visualize with Heatmap(seaborn):\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(correlation_matrix,annot=True,cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "c. Other Correlation Methods: \n",
    "\n",
    "We can also specify methods like:\n",
    "\n",
    "\"pearson\" - default, for linear relationships\n",
    "\"kendall\" - for ordinal or non-parametric data\n",
    "\"spearman\" - for ranked data\n",
    "\n",
    "df.corr(method='spearman')\n",
    "\n",
    "15. What is causation? Explain difference between correlation and causation with an example.\n",
    "\n",
    "-> Causation means that one variable directly affects another - a change in one variable produces a change in the other.\n",
    "causation=cause and effect.\n",
    "\n",
    "Difference between correlation and causation are:\n",
    "\n",
    "Correlation:\n",
    "a. Shows a statistical relationship between two variables.\n",
    "b. Change in one variable is associated with change in another.\n",
    "c. Does not imply one variable causes the other to change.\n",
    "d. Can be positive, negative, or zero.\n",
    "e. May be due to a third(confounding) variable.\n",
    "\n",
    "Example: Ice cream sales and drowning rates both increase in summer(due to heat).\n",
    "\n",
    "Causation:\n",
    "a. Indicates a cause-and-effect relationship.\n",
    "b. Change in one variable directly causes change in another.\n",
    "c. Requires controlled experiments or strong evidence.\n",
    "d. Always has a directional influence(from cause to effect).\n",
    "\n",
    "Example: Smoking causes lung cancer(proven by medical research).\n",
    "\n",
    "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "\n",
    "-> An optimizer is an algorithm or method used in machine learning and deep learning to adjust the model's \n",
    "parameters(like weights and biases) during training, with the goal of minimizing the loss function. \n",
    "The loss function measures how well the model is performing - the optimizer tries to find the best parameters \n",
    "that reduce this error.\n",
    "\n",
    "Different types of optimizers:\n",
    "a. Gradient Descent(GD):\n",
    "-> The most basic optimizer.\n",
    "-> Updates model parameters by moving them in the direction of the negative gradient of the loss function.\n",
    "-> Calculates gradients on the entire dataset for each update.\n",
    "\n",
    "Example: Used in simple linear regression.\n",
    "\n",
    "b. Stochastic Gradient Descent(SGD):\n",
    "-> Similar to Gradient Descent but updates parameters using one training example at a time.\n",
    "-> Faster updates but noisier(less stable).\n",
    "-> Useful for large datasets.\n",
    "\n",
    "Example: Used in neural networks where data is too big to process at once.\n",
    "\n",
    "c. Mini-batch Gradient Descent:\n",
    "-> A hybrid between GD and SGD.\n",
    "-> Updates parameters based on a small random batch of data(mini-batch) instead of the whole dataset or a single example.\n",
    "-> Balances speed and stability.\n",
    "\n",
    "Example: Commonly used in deep learning frameworks like TensorFlow and PyTorch.\n",
    "\n",
    "d. Momentum:\n",
    "-> Accelerates SGD by adding a momentum term that helps the optimizer keep moving in the same direction.\n",
    "-> Helps avoid getting stuck in local minima and speeds up convergence.\n",
    "\n",
    "Example: Often combined with SGD in training deep neural networks.\n",
    "\n",
    "e. RMSprop(Root Mean Square Propagation):\n",
    "-> Adjusts the learning rate adaptively for each parameter based on recent gradients.\n",
    "-> Divides the learning rate by a moving average of the magnitudes of recent gradients.\n",
    "-> Works well for non-stationary objectives(changing loss landscape).\n",
    "\n",
    "Example: Popular for recurrent neural networks(RNNs).\n",
    "\n",
    "f. Adam(Adaptive Moment Estimation):\n",
    "-> Combines ideas from Momentum and RMSprop.\n",
    "-> Maintains moving averages of both gradients and squared gradients.\n",
    "-> Automatically adjusts learning rates for each parameter.\n",
    "-> Often works well in practice and is widely used.\n",
    "\n",
    "Example: Default optimizer for many deep learning tasks.\n",
    "\n",
    "17. What is sklearn.linear_model?\n",
    "\n",
    "-> sklearn.linear_model is a module in the scikit-learn Python library that provides classes \n",
    "and functions to implement various linear models for regression and classification tasks.\n",
    "\n",
    "It offers:\n",
    "a. It contains algorithms that model the relationship between input features and \n",
    "a target variable assuming a linear relationship.\n",
    "b. It supports both regression(predicting continuous values) and classification(predicting categories) problems.\n",
    "c. The module includes simple and advanced linear models, along with tools for regularization to prevent overfitting.\n",
    "\n",
    "Example:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)  # Train the model\n",
    "predictions=model.predict(X_test)  # Predict outcomes\n",
    "\n",
    "18. What does model.fit() do? What arguments must be given?\n",
    "\n",
    "-> The fit() method in machine learning models (including scikit-learn models) is used to train the model \n",
    "on the provided data. When we call model.fit(), the algorithm:\n",
    "a. Learns the relationship between the input features and the target variable.\n",
    "b. Finds the best parameters(e.g., weights in linear regression) that minimize the error or loss function.\n",
    "c. Prepares the model to make predictions on new/unseen data.\n",
    "\n",
    "The most common arguments passed to fit() are:\n",
    "\n",
    "a. X — The input data(features):\n",
    "\n",
    "-> Usually a 2D array-like structure(e.g., NumPy array, pandas DataFrame).\n",
    "-> Shape: (number of samples, number of features).\n",
    "\n",
    "b. y — The target variable (labels or values to predict):\n",
    "\n",
    "-> 1D array-like for regression or binary/multiclass classification.\n",
    "-> Shape: (number of samples,).\n",
    "\n",
    "Example:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "# Sample data\n",
    "X=np.array([[1,2],[2,3],[3,4],[4,5]])  # Features\n",
    "y=np.array([3,5,7,9])                  # Target variable\n",
    "model=LinearRegression()\n",
    "model.fit(X,y)  # Train the model on X and y\n",
    "\n",
    "19. What does model.predict() do? What arguments must be given?\n",
    "\n",
    "-> The predict() method is used to make predictions using the trained machine learning model. \n",
    "After we have trained our model with fit(), calling predict() applies the learned patterns \n",
    "to new input data to estimate the output(target values or classes).\n",
    "\n",
    "The most common arguments passed to predict() are:\n",
    "a. X - The input data(features) for which we want predictions:\n",
    "-> Typically a 2D array-like structure(NumPy array, pandas DataFrame).\n",
    "-> Shape: (number of samples, number of features).\n",
    "b. No target(y) is needed because we want the model to predict these.\n",
    "\n",
    "Example:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "# Training data\n",
    "X_train=np.array([[1,2],[2,3],[3,4]])\n",
    "y_train=np.array([3,5,7])\n",
    "# Train the model\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "# New data for prediction\n",
    "X_new=np.array([[4,5],[5,6]])\n",
    "# Predict target values for new data\n",
    "predictions=model.predict(X_new)\n",
    "print(predictions)\n",
    "\n",
    "20. What are continuous and categorical variables?\n",
    "\n",
    "-> Continuous Variables:\n",
    "a. These are quantitative variables that can take any numerical value within a range.\n",
    "b. They are measurable and can include decimals or fractions.\n",
    "\n",
    "Examples:\n",
    "Height(e.g., 172.5 cm)\n",
    "Temperature(e.g., 36.6 °C)\n",
    "Salary(e.g., ₹55,000.75)\n",
    "\n",
    "Categorical Variables:\n",
    "a. These are qualitative variables that represent categories or groups.\n",
    "b. They have discrete values and cannot be measured on a numerical scale.\n",
    "\n",
    "Types:\n",
    "Nominal: No inherent order(e.g., gender, color)\n",
    "Ordinal: With an order or ranking(e.g., education level, satisfaction rating)\n",
    "\n",
    "Examples:\n",
    "Gender(Male, Female)\n",
    "Marital Status(Single, Married)\n",
    "Shirt Size(Small, Medium, Large)\n",
    "\n",
    "21. What is feature scaling? How does it help in Machine Learning?\n",
    "\n",
    "-> Feature scaling is the process of normalizing or standardizing the range of independent variables(features) \n",
    "in our data. It transforms features so they have similar scales or distributions.\n",
    "\n",
    "It helps in machine learning:\n",
    "a. Many ML algorithms(like gradient descent, k-nearest neighbors, SVM, and neural networks) \n",
    "are sensitive to the scale of features.\n",
    "b. Features with larger ranges can dominate the learning process, causing biased results.\n",
    "c. Scaling ensures all features contribute equally, improving:\n",
    "-> Model convergence speed(especially for gradient-based methods).\n",
    "-> Model performance and accuracy.\n",
    "-> Interpretability when comparing feature effects.\n",
    "\n",
    "22. How do we perform scaling in Python?\n",
    "\n",
    "-> We can perform feature scaling easily in Python using scikit-learn's preprocessing module. \n",
    "\n",
    "The common methods are:\n",
    "\n",
    "a. Min-Max Scaling:\n",
    "Scales data to a range between 0 and 1.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "data=np.array([[10, 200],\n",
    "                 [20, 300],\n",
    "                 [30, 400]])\n",
    "scaler=MinMaxScaler()\n",
    "scaled_data=scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "\n",
    "b. Standardization(Z-score scaling):\n",
    "Centers data to mean zero and scales to unit variance.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "data=np.array([[10, 200],\n",
    "                 [20, 300],\n",
    "                 [30, 400]])\n",
    "scaler=StandardScaler()\n",
    "scaled_data=scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "\n",
    "c. Normalizer:\n",
    "\n",
    "Scales each sample(row) to have unit norm(length 1), converting it into a unit vector.\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "data=np.array([[4, 1],\n",
    "                 [1, 2],\n",
    "                 [3, 3]])\n",
    "scaler=Normalizer()\n",
    "normalized_data=scaler.fit_transform(data)\n",
    "print(normalized_data)\n",
    "\n",
    "Steps to perform scaling:\n",
    "a. Create the scaler object.\n",
    "b. Apply .fit_transform() on our data.\n",
    "c. Get scaled data ready for modeling.\n",
    "\n",
    "23. What is sklearn.preprocessing?\n",
    "\n",
    "-> sklearn.preprocessing is a module in the Scikit-learn(sklearn) library that provides tools \n",
    "for preparing(or preprocessing) data before feeding it into a machine learning model.\n",
    "\n",
    "This module helps to:\n",
    "\n",
    "a. Scale features(e.g., standardize values)\n",
    "b. Encode categorical variables\n",
    "c. Normalize data\n",
    "d. Handle missing or unevenly distributed data\n",
    "\n",
    "24. How do we split data for model fitting (training and testing) in Python?\n",
    "\n",
    "-> We use the train_test_split function from Scikit-learn(sklearn) to divide the dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Assuming X=features, y=labels/target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "Parameters:\n",
    "test_size=0.2 -> 20% data for testing, 80% for training.\n",
    "random_state=42 -> ensures reproducibility.\n",
    "shuffle=True by default -> shuffles before splitting.\n",
    "\n",
    "25. Explain data encoding?\n",
    "\n",
    "-> Data encoding in machine learning is the process of converting categorical variables(non-numeric data) \n",
    "into a numeric format so that algorithms can process them effectively. Since most machine learning models \n",
    "work with numerical data, encoding is a crucial step in data preprocessing.\n",
    "\n",
    "Encoding is important because:\n",
    "a. Algorithms like linear regression, logistic regression, SVMs, etc., require numeric input.\n",
    "b. Encoding allows models to interpret categories meaningfully.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
